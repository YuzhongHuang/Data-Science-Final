{
  "name": "Data-science-final",
  "tagline": "",
  "body": "# Project Story 2\r\n\r\n#### Yuzhong Huang, Jay Woo, Wilson Tang\r\n\r\n## Overview\r\n\r\nAt the end of our last phase we were working on finishing our data exploration and reformating, feature learning neural network, feedforward prediction neural network, and visualizations of our feature learning model. During this phase, we built the feature learning systems, generated the learned features and recoded the icd-9 features. Right now, we are deploying the learned features to our prediction neural network to predict diabetes. \r\n\r\n## Data Exploration + Reformatting\r\n\r\nAfter extensive exploration of the data, we found that the lab and medication data was too diverse and weakly linked to our patients to utilize in our project. We decided to exclude the two and focus on the ICD-9 codes and general information such as age, gender, weight, height and etc.\r\n\r\nWe tested 2 groupings for ICD-9 codes: 17 broad super-categories and 107 specific subcategories with our feature learning neural network (narrowing from 13,000 possible codes). \r\n\r\nWe ended up deciding that 107 subcategories represented the ICD-9 data properly for our network.\r\n\r\n## Feature Learning Neural Network\r\n\r\n![enter image description here](https://lh3.googleusercontent.com/-aLe_ZPvqpN8/VxkWaV0IsuI/AAAAAAAAAgw/oPE6EU8aet41ltkh9zLWfSrwDCbG1SCbACLcB/s0/autoencoder.png \"autoencoder.png\")\r\n\r\nThe autoencoder is a special kind of neural network that effectively reduces the dimensionality of the input. For example, by reducing 6 features to 4 features and then reconstructing the 6 features from 4 features, the network is forced to combine dimensions that are highly related to each other. In this way, the network could potentially extract some important features out from the raw inputs. \r\n\r\nWe built a single-layer, denoising autoencoder and ran a simple example on it by taking 27 features and outputting 22 learned features. Also, we added 30% noise by setting 30 percent of the input features to be 0. By doing that, the network cannot rely much on each individual features, since some of the features are not reliable. Instead, the network is forced to learn the relationships between features to predict the value of noise by using other values. \r\n\r\nWe trained 100 epochs over our network and compared the reconstructed features with the original ones. The result is quite inspiring: the standard deviation of the 27 reconstructed features is about 0.04. Since all of the input features are normalized and are in range of 0 to 1. With a standard deviation of 0.04, the model is doing a good job reconstructing the 27 features from the 22 learned features. But we still need to test how good are the 22 features in representing the data with our prediction network. \r\n\r\n## Prediction Neural Network\r\n\r\n![enter image description here](https://lh3.googleusercontent.com/-RMYxIditqtI/VxkWfqiDg1I/AAAAAAAAAg4/4sbdHwIk6x8TIuKPd4tc1KQXuN9w4WcnQCLcB/s0/neural_net.jpeg \"neural_net.jpeg\")\r\n\r\nIn order to predict which patients have diabetes, we trained a classic feed-forward neural network model on the medical record data. Each neuron in one layer is attached to everything in the next layer, such that the “activation energy” from an adjacent neuron propagates through the network. The neurons in the hidden layer(s) take in energy from all the preceding neurons, and if it reaches above a certain threshold, the node becomes activated. In the context of our dataset, we put in our features after putting them through the autoencoder. If the patient has diabetes, we would expect to see the output layer have a high value, and many of the hidden layer nodes would probably be activated. Otherwise, the output will be small.\r\n\r\nWe implemented the neural network using Theano’s multilayer perceptron tutorial. We noticed that the model had an incredibly low error rate, even with randomized weights, but this was most likely due to the fact that there weren’t a lot of patients who have diabetes in the dataset (less than 1%). In the future, we are considering predicting for a more prevalent condition like hypertension, which many more the patients in our dataset actually have.\r\n\r\n## Plans for Final Phase \r\n\r\nOur next step would be compare the prediction result of learned features with prediction result of original features. After that, we will build a stacked denoising autoencoder that learns higher level features. We hope to synthesize our prediction system and make the model general that can take a target name, and the system will generates a model for predicting the given target. We would also like to get some visualizations made by the end of the project, so that we can present our work very cleanly. One of our options is to visualize one of the hidden layers in the neural network to see what the model weights more heavily. Since our dataset does not comprise of images, it may be more difficult to make a clean graph.\r\n",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}